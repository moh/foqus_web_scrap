apify
micro services python
piplines dans Azure

Google database openimage v4 


==================================
NOTE
==================================

---------------------------------------------------
First generic test method on tryn.fit - 17/06/2020:
---------------------------------------------------

First test on extracting links and identifying product and page link by the bot by itself, the method consists of : 

1 - Remove header and footer element, since they contain usually links related to website contact, faq ... 
2 - Get product links by extracting the element "a" that has "img" as a child, this method works on "tryn.fit"
3 - Get other links, and treat them as page links.
4 - Call the product page and check if it is really a product page, the verification of product page is based on the presence of a class ( this is only for tryn.fit ).
5 - if it is a product page, then extract data.
6 - not a product page, then yield a request to be treated as a normal page.

The PROBLEM that we encountered in this approach is that the bot keep clicking on link of format ".../?add_to_wishlist=1233", this link is used to add an item to "wish list", so a link that neither page of product or set of products.


Proposed SOLUTION : 
===================

Most websites have links to contact, faq, whishlist, login ... so the proposed solution is to form a list of common words that are used by websites to access those different section, and then get the links that doesn't contain any words.

 + Will filter many links from many websites, as they use the same words.

 - A change in the website link will make this feature useless.

 - Require large list of words to be efficient.


---------------------------------------------
Second generic test on tryn.fit - 18/06/2020:
---------------------------------------------

This test is to identify the nature of the page we are scraping, in this project we have only two pages that we are interested in, home page : where there is a list of products without much details, and product page where there is the details about one product, and lists of other related products ( in most sites ).

This test takes a csv file as input, the rows represents the sites that we want to crawl, and each rows have two links, the first link represent a home link, and the second link represent a product page link of any product. 

NOTE : For the home page, it is not necessarily the home page like "https://tryn.fit/", 
------ the best options for the home page, is a page that share its form and design with so many other pages, for example : "https://tryn.fit/categorie-produit/chemises-homme/"


How it works : 
--------------
1 - the program gets the page of the home page, then extract all the classes in the home page.
2 - Then it gets the page of the product page, and extract all the classes in the product pages.
3 - we extract the classes that may be specific only to the home page ( classes of home page - classes of product page)
4 - we extract the classes that may be specific only to the product pages ( classes of product pages - classes of home page )
5 - To identify if a page is either a home or product page, we extract the classes in that page,
then we match the classes with the lists of home and product page.
6 - if the result of the match in both pages is less then MIN_RATIO then the page is neither a product nor a home page.

This method has worked perfectly on tryn.fit
Worked on many sites.


NOTE TO update : To identify the product page, we should not only take into account the url base, but ---------------- also the whole netloc.

